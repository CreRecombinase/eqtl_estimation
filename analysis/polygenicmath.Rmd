---
title: "Deriving Polygenic Approximation"
author: "Knoblauch Nicholas"
date: 2017-07-12
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->


# Background

##General properties of compound normal

If $$x|\mu \sim N(A\mu,\Sigma)$$ and $$\mu \sim N(\rho,\Lambda)$$ then the marginalized form of $x$ is $$x \sim N(A\rho,A \Lambda A^{T} + \Sigma)$$

## RSS polygenic prior on $\beta$

According to the RSS likelihood:

$$\hat{\beta}|\beta \sim N(SRS^{-1}\beta,SRS)$$ Where $R$ is the population LD matrix, and $S$ is a diagonal matrix with entires $S_{jj}=\frac{1}{\text{se}(\hat{\beta}_j)}$  This means that if $\beta \sim N(0,I\sigma_{\beta}^2)$ , then we can obtain the the marginalized form of $\hat{\beta}$ by substituting $$A=SRS^{-1}$$ $$\rho=0$$ $$\Lambda=I\sigma_{\beta}^2$$  and $$\Sigma=SRS$$



$$\hat{\beta} \sim N(0,(SRS^{-1})I\sigma_{\beta}^2(SRS^{-1})^{T}+SRS)=N(0,\sigma_{\beta}^2 SRS^{-2}RS+SRS)$$

## Estimating $\sigma_{\beta}^2$

I've simulated data under 10 different values of $\sigma_{\beta}^2$, and will try to estimate these parameters using a grid search in a modified version of `RSS`, and a grid search using a multivariate normal distribution.
```{r,echo=F,message=F, dataload}
library(rssr)
library(ggplot2)
library(tidyr)
data("betahat_mat_norm")
data("R_panel")
data("se_mat_norm")
data("tparam_df_norm")

library(dplyr)
R_paneld <- as.matrix(R_panel)
R <- as.matrix(R_paneld)
sigb_range <- c(min(tparam_df_norm$tsigb),max(tparam_df_norm$tsigb))
paramdf <- data.frame(sigb=seq(sigb_range[1]/2,sigb_range[2]*2,length.out = 100))
ng <- ncol(betahat_mat_norm)
p <- nrow(R_panel)
alphap=rep(1,p)
mup <- rmu(p)

tresl_vi <- list()
tresl_af <- list()
```

```{r echo=F,message=F, norm_function}
aform_rss <- function(se,R,betahat,sigb_grid,progress=F){
  library(progress)
  library(dplyr)
  S <- diag(se)
  Si <- diag(1/se)
  SRSi <- S%*%R%*%Si
  SRSis <- SRSi%*%t(SRSi)
  SRS <- S%*%R%*%S
  p <- length(betahat)
  dvec <- numeric(length(sigb_grid))

  for( i in 1:length(sigb_grid)){
    dvec[i] <- mvtnorm::dmvnorm(x=c(betahat),mean = rep(0,p),sigma = sigb_grid[i]*SRSis+SRS,log = T)
 
  }
  return(data_frame(lnZ=dvec,sigb=sigb_grid))
  
}
```

```{r,echo=F,eval=F, noeval}

progress <- T
if(progress){
  pb<- progress_bar$new(total=ng)
}

tresl_vi <- list()
tresl_af <- list()
for(i in 1:ng){
  siris <- SiRSi_d(R_paneld,Si = 1/se_mat_norm[,i])
  sirisr <- c(siris%*%(alphap*mup))
  
  tr <- system.time(tresl_vi[[i]] <- grid_search_rss_varbvsr_norm(SiRiS = siris,
                                                sigma_beta = paramdf$sigb,
                                                betahat = betahat_mat_norm[,i],
                                                se = se_mat_norm[,i],talpha0 = alphap,
                                                tmu0 = mup,tSiRiSr0 = sirisr,tolerance = 1e-3,
                                                itermax = 200,verbose = F,lnz_tol = T)  %>% mutate(fgeneid=i))
  tresl_vi[[i]] <- mutate(tresl_vi[[i]],timeres=tr[3])
  tr <- system.time(tresl_af[[i]] <- aform_rss(se = se_mat_norm[,i],R = R,betahat = betahat_mat_norm[,i],sigb_grid = paramdf$sigb,progress=F) %>% mutate(fgeneid=i))
  tresl_af[[i]] <- mutate(tresl_af[[i]],time=tr[3])
  if(progress){
  pb$tick()
}
}


af_df <- bind_rows(tresl_af)
vi_df <- bind_rows(tresl_vi) %>% select(sigb,lnZ,fgeneid,time=timeres)
b_df <- inner_join(af_df,vi_df,by=c("sigb","fgeneid"),suffix=c("_af","_vi")) %>% inner_join(tparam_df_norm)
saveRDS(b_df,file = "../output/norm_norm.RDS")
m_df <- group_by(b_df,fgeneid) %>% summarise(max_af=sigb[which.max(lnZ_af)],max_vi=sigb[which.max(lnZ_vi)],time_af=time_af[1],time_vi=time_vi[1]) %>% ungroup() %>% inner_join(tparam_df_norm)
saveRDS(m_df,file="../output/m_norm.RDS")

```

## Results
```{r,echo=F,message=F, plot1}
b_df<- readRDS("../output/norm_norm.RDS")
m_df <- readRDS("../output/m_norm.RDS")

best_est <- select(m_df,-tsigb,-tpi,-replicate,-tlogodds,-tpve,-time_af,-time_vi) %>% gather(method,max,-fgeneid) %>% mutate(method=ifelse(method=="max_af","Analytical","Variational")) %>% inner_join(tparam_df_norm) %>% select(fgeneid,method,max,tsigb)

ggplot(best_est,aes(x=tsigb,y=max,col=method))+geom_point()+ggtitle("Estimating sigma_beta","Variational Inference vs Analytical Form")+xlab("True Sigma_beta")+ylab("Estimate")+geom_smooth()
```


Both methods seem to give pretty similar results.  Which one has a lower relative error (defined as $\frac{|x-y|}{|x|+|y|}$)?
```{r ,echo=F,message=F,plot2}
mutate(best_est,rel_error=abs(max-tsigb)/(abs(max)+abs(tsigb))) %>% ggplot(aes(x=tsigb,y=rel_error,col=method))+geom_point()+ggtitle("Relative Error Estimating sigma_beta","Variational Inference vs Analytical Form")+xlab("True Sigma_beta")+ylab("Relative Error")+geom_smooth()
```

## Performance
Surprisingly, the variational inference method is several times faster.  This is likely because the variational method doesn't have to compute the determinant of the covariance matrix, which is an $O(n^3)$ operation (at best)
```{r,echo=F,message=F, plot3}
nlabs <- c("Analytical","Variational")
names(nlabs) <- c("af","vi")
m_time <- select(m_df,time_af,time_vi) %>% gather(method,time) %>% mutate(method=nlabs[gsub("time_(.+)","\\1",method)])
ggplot(m_time,aes(x=method,y=log10(time)))+geom_boxplot()+ggtitle("Time to perform inference","100 grid values,982 SNPs")+ylab("log10(seconds)")
```

## Directly Comparing the lower-bound to the log-likelihood
  Finally, This is a comparison of the normalized log-likelihood to the normalized variational lower-bound. Normalization in this context means exponentiating the log lower-bound values (or log-likelihoods), and dividing by the sum of these exponentiated values (using the log-sum-exp trick).

```{r,echo=F, plotnormvb}
group_by(b_df,fgeneid) %>% mutate(w_af=normalizeLogWeights(lnZ_af),w_vi=normalizeLogWeights(lnZ_vi)) %>% ungroup()  %>%
  ggplot(aes(x=w_af,y=w_vi,col=factor(fgeneid)))+geom_point()+facet_wrap(~tsigb,scales = "free")+ggtitle("Normalized Variational Lower Bound vs Normalized Log-Likelihood","10 values of sigma_beta")+ylab("Normalized Variational Lower Bound")+xlab("Normalized Marginalized Log-Likelihood")
```

